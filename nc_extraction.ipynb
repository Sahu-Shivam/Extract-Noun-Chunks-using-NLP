{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a1ab90ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "4c4ce841",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.listdir('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "c31ba62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_articles = []\n",
    "for i in directory[:20]:\n",
    "    with open('data/'+i, 'rb') as f:\n",
    "        file = json.load(f)\n",
    "        list_of_articles.append(file['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "bcdc8e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extracting_Noun_Chunks:\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def preprocess_article(self,article):\n",
    "\n",
    "            #-------Filtering using regex-------#\n",
    "            # Steps followed:\n",
    "            # 1. Passing in text in smallcase.\n",
    "            # 2. Removing numbers.\n",
    "            # 3. Removing any extra tabs.\n",
    "            article = re.sub('\\s\\s+', ' ',(re.sub('[^ a-z]', ' ', article.lower())))\n",
    "\n",
    "\n",
    "            #-------Extracting the words that actually have meaning - basically removing stopwords-------#\n",
    "            # Steps:\n",
    "            # 1. Pulling out distinct stopwords.\n",
    "            # 2. Creating a list of words that are not stopwords.\n",
    "            # 3. Converting the list to a string\n",
    "            article = article.split(' ')\n",
    "            article = \" \".join([word for word in article if word not in set(stopwords.words('english'))])\n",
    "\n",
    "            return article\n",
    "    \n",
    "    \n",
    "    def extract_noun_chunks(self, article):\n",
    "        \n",
    "        #-------Creating a Spacy Document-------#\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        doc = nlp(article)\n",
    "\n",
    "        \n",
    "        #-------Extracting Noun-Chunks based on combination of 2 or 3 words (Bigrams/Trigrams)-------#\n",
    "        noun_chunks = [i.text for i in list(doc.noun_chunks) if (1 < len(i.text.split(' ')) < 4)]\n",
    "        return noun_chunks\n",
    "    \n",
    "    \n",
    "    def word_makes_sense(self, noun_chunks):\n",
    "        \n",
    "        n_chunk = []\n",
    "        \n",
    "        for word in noun_chunks:\n",
    "            #---Splitting seperate words and creating a list---#\n",
    "            word = word.split(' ')\n",
    "            \n",
    "            #---Checking if the word makes sense and adding the words that make sense to new list---#\n",
    "            temp = [i for i in word if i in set(words.words())]\n",
    "        \n",
    "        #---Comparing new list to old list---#\n",
    "            if len(temp) < len(word):\n",
    "                #---If the length is same, i.e. all the words in the old list makes sense, then returning the word---# \n",
    "                n_chunk.append(' '.join(word))\n",
    "        \n",
    "        return n_chunk\n",
    "\n",
    "    \n",
    "    def postprocess_noun_chunks(self, noun_chunks):\n",
    "        \n",
    "        #-------Deduplicating Noun-Chunks and removing words that doesn't make sense-------#\n",
    "        noun_chunks = self.word_makes_sense(list(set(noun_chunks)))\n",
    "        \n",
    "        return noun_chunks\n",
    "    \n",
    "    \n",
    "    def tfidf_vectorizer(self, all_articles):\n",
    "       \n",
    "        #-------Calulating Term Frequency, Inverse Document Frequency (Tfidf) to get top noun-chunks-------#\n",
    "        # Steps followed:\n",
    "        # 1. Instantiating TfidfVectorizer().\n",
    "        # 2. Fitting and transforming all the articles on the vectorizer.\n",
    "        # 3. Calculating the scores.\n",
    "        # 4. Getting feature words (sorted by index) selected from the raw documents\n",
    "        vectorizer = TfidfVectorizer(ngram_range=(2,3))\n",
    "        \n",
    "        scores = vectorizer.fit_transform(all_articles)  # has a shape of (20, 19152)\n",
    "        \n",
    "        scores = scores.toarray().sum(axis=0)            # scores = summation of all the rows(20).\n",
    "\n",
    "        total_words = vectorizer.get_feature_names()        \n",
    "        \n",
    "        return scores, total_words\n",
    "    \n",
    "    \n",
    "    def __call__(self, list_of_articles):\n",
    "        \n",
    "        print('Please wait while the model finds the top 10 Noun-Chunks...\\n')\n",
    "        noun_chunks = []\n",
    "        \n",
    "        # Preprocessing and extracting Noun_Chunks\n",
    "        for article in list_of_articles:\n",
    "            \n",
    "            #-----Pre-Procesing-----#\n",
    "            article = self.preprocess_article(article)\n",
    "            \n",
    "            #-----Extracting Noun-Chunks-----#\n",
    "            noun_chunks.extend(self.extract_noun_chunks(article))\n",
    "            \n",
    "            #-----Post-Procesing-----#            \n",
    "            noun_chunks = self.postprocess_noun_chunks(noun_chunks)\n",
    "            \n",
    "            #-----Calculating tfidf score and extracting the total words-----#                        \n",
    "            scores, total_words = self.tfidf_vectorizer(list_of_articles)\n",
    "\n",
    "            #-----Creating a list of noun_chunks and their scores-----#\n",
    "            noun_chunks_score = []\n",
    "            for i in noun_chunks:\n",
    "                try:\n",
    "                    total_words_index = total_words.index(i)\n",
    "                    noun_chunks_score.append([i, scores[total_words_index]])\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            #-----Sorting the score in descending order-----#\n",
    "            nc_score = sorted(noun_chunks_score, key=lambda x:-x[1])  # -x[1] sorts the list in descending order\n",
    "            \n",
    "            #-----Returning the top 10 noun_chunks-----#\n",
    "            return [i[0] for i in nc_score[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "9c516ac8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait while the model finds the top 10 Noun-Chunks...\n",
      "\n",
      "Top 10 Noun_Chunks are as below:\n",
      " ['trail blazers', 'san mateo', 'season ticket renewals', 'organizational records', 'leed gold recertification', 'basketball games marketo', 'needed technology', 'concerts family', 'deeper level', 'small companies']\n"
     ]
    }
   ],
   "source": [
    "#-----Instantiating the model-----#\n",
    "NC = Extracting_Noun_Chunks()\n",
    "\n",
    "#-----Creating Object-----#\n",
    "noun_chunks = NC(list_of_articles)\n",
    "print(\"Top 10 Noun_Chunks are as below:\\n\",noun_chunks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
